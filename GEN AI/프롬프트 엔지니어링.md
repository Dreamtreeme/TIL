## 프롬프트 엔지니어링
- 생성형 AI로 부터 원하는 결과를 얻기 위해 명령을 설계하고 구조화 하는기술
### OpenAI API

25년 3월 11일, 인공지능 Agent로 발전하기 위해 개인용PC에 접근할 수 있게만듦.

1. 주피터 노트북 에서 openAPI key 넣음
2. openai 라이브러리에서 OpenAI 임포트해 인스턴스 생성
3. https://platform.openai.com/docs/examples 에서 Parse unstructured data 코드 복사
4. 시스템 text에 페르소나설정
5. 유저 text에 비정형데이터 문자열로 넣음
6. 응답이 항상 문자열+백틱이 오기 때문에 파이썬에서 분리하고 객체로 만들어야함.
7. 객체로 만들기위해 json라이브러리에서 loads()메서드와 dumps() 메서드 활용
8. 
    1. 백틱을 먼저 제거하기 위해 시스템 text에 강조를함
        답변 예시:
        추가 요청

    2. type 설정
    input 중괄호 뒤에 ,하고
        text = {
        'format': {
            "type": "json_object"
            }
        },

9. 복잡한 구조의 JSON 파싱 요청
json 스키마를 사전정의해 제한시킬 수 있음

    OpenAi platform 에서 Structured Outputs -> Chat Completions 확인
    class FruitInfoJsonFormat(BaseModel):
        fruits: list[str]
        num_of_fruits: int
        ref: str
    response = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[
            {"role": "system", "content": "당신은 구조화 되지 않은 데이터를 JSON 형식으로 변환하는 AI입니다. 사용자가 과일에 대한 설명을 제공하면, 과일 이름과 과일 수, 출처를 JSON 형식으로 추출하세요."},
            {"role": "user", "content": user_prompt},
        ],
        response_format=FruitInfoJsonFormat,
    )

10. GPT가 생성한 추가 데이터를 장고에 저장하려면?
save(commit = False)
사용자의 요청을 중간에 stop하는 코드
중간에 gpt 응답을 인스턴스에 할당후 저장


### 웹크롤링
1. 웹페이지 다운로드
2. 페이지 파싱
3. 링크 추출 및 다른 페이지 탐색
4. 데이터 추출 및 저장

#### 필수 라이브러리
- requests
- BeautifulSoup
- Selenium

pip install requests beautifulsoup4 selenium



## 웹 크롤링을 통한 댓글 데이터 수집 및 분석 프로젝트 과정 정리
1. 프로젝트 개요
이 프로젝트는 사용자가 입력한 회사 이름에 대해 웹 크롤링을 수행하여 주식 관련 댓글 데이터를 수집하고, 이를 데이터베이스에 저장하며, 추가 분석(예: 감정 분석)을 가능하게 하는 애플리케이션 개발 과정을 다룹니다. 동적 웹 페이지 크롤링을 위해 Django와 Selenium을 활용하며, 데이터 파싱에는 BeautifulSoup을 사용합니다.

2. 주요 기능
사용자 입력 처리: 사용자가 회사 이름을 입력하면 해당 데이터를 기반으로 크롤링 실행.
웹 크롤링: Selenium으로 동적 웹 페이지에서 댓글 데이터 추출.
데이터 저장: Django ORM을 활용해 크롤링 데이터를 데이터베이스에 저장.
결과 표시: 수집된 데이터를 사용자에게 웹 템플릿으로 출력.
3. 개발 환경
프레임워크: Django (웹 애플리케이션 구축)
크롤링 도구: Selenium (ChromeDriver로 동적 페이지 제어)
파싱 도구: BeautifulSoup (HTML 파싱)
데이터베이스: Django ORM (기본 SQLite, 필요 시 MySQL/PostgreSQL로 확장 가능)
추가 도구: Python, re (정규 표현식), WebDriverWait (동적 요소 대기)
4. 개발 과정 및 코드 흐름
4.1. 초기 설정
데이터베이스에서 기존 댓글 데이터를 최신순으로 가져오고, 결과 및 분석 변수를 초기화.
python

접기

자동 줄바꿈

복사
comments = Comment.objects.all().order_by('-created_at')  # 최신 댓글 순 정렬
result = None
analysis = None
sentiment_analysis_result = None
4.2. 사용자 입력 처리 (POST 요청)
사용자가 회사 이름을 입력하지 않은 경우, 오류 메시지 출력 후 템플릿 렌더링.
python

접기

자동 줄바꿈

복사
if not company_name:
    result = "회사 이름을 입력하세요!"
    return render(request, 'crawlings/stock_finder.html', {
        'comments': comments, 
        'result': result, 
        'analysis': analysis, 
        'sentiment_analysis_result': sentiment_analysis_result
    })
4.3. Selenium 초기화
Chrome 브라우저를 열고 크롤링 대상 페이지로 이동.
python

접기

자동 줄바꿈

복사
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument("--headless")  # 필요 시 헤드리스 모드 활성화
driver = webdriver.Chrome(options=chrome_options)
driver.get('https://tossinvest.com/')  # 크롤링 대상 URL
4.4. 검색 실행
검색 버튼 클릭 후 회사 이름 입력 및 검색 실행.
python

접기

자동 줄바꿈

복사
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys

# 검색 버튼 클릭
search_button = WebDriverWait(driver, 10).until(
    EC.element_to_be_clickable((By.CLASS_NAME, "u09klc0"))
)
search_button.click()

# 검색어 입력 및 실행
search_input = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, "input[data-section-name='검색']"))
)
search_input.click()
search_input.send_keys(company_name)
search_input.send_keys(Keys.ENTER)
4.5. 커뮤니티 페이지 이동
커뮤니티 탭으로 이동해 댓글 데이터가 있는 페이지로 전환.
python

접기

자동 줄바꿈

복사
community_button = WebDriverWait(driver, 10).until(
    EC.element_to_be_clickable((By.CSS_SELECTOR, "button[data-contents-label='커뮤니티']"))
)
community_button.click()
4.6. 주식 코드 추출
페이지 소스를 BeautifulSoup으로 파싱해 6자리 주식 코드를 추출.
python

접기

자동 줄바꿈

복사
from bs4 import BeautifulSoup
import re

soup = BeautifulSoup(driver.page_source, 'html.parser')
code = '코드 없음'
for span in soup.find_all('span'):
    text = span.get_text().strip()
    if re.fullmatch(r'\d{6}', text):  # 6자리 숫자 패턴 확인
        code = text
        break
4.7. 댓글 데이터 크롤링
댓글 섹션을 찾아서 각 댓글의 내용과 날짜를 추출.
python

접기

자동 줄바꿈

복사
article_section = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, "section[data-section-name='게시글']"))
)
soup = BeautifulSoup(article_section.get_attribute('outerHTML'), 'html.parser')
comment_articles = soup.select("article.comment.xdogm41")
extracted_comments = []

for article in comment_articles:
    try:
        time_tag = article.find('time')
        datetime_str = time_tag['datetime'] if time_tag else None
        comment_span = article.select_one('span.tw-1r5dc8g0._60z0ev1')
        comment_text = comment_span.get_text().strip().replace('\n', '') if comment_span else '내용 없음'

        if datetime_str:
            Comment.objects.create(
                company_name=company_name,
                stock_code=code,
                content=comment_text
            )
            extracted_comments.append(comment_text)
    except Exception as e:
        print(f'댓글 처리 중 오류: {e}')
4.8. 결과 처리 및 리소스 해제
크롤링 완료 메시지 설정 후 브라우저 종료.
python

접기

자동 줄바꿈

복사
result = f"'{company_name}' 댓글 크롤링 및 저장 완료!"
driver.quit()  # 리소스 해제를 위해 필수
return render(request, 'crawlings/stock_finder.html', {
    'comments': comments, 
    'result': result, 
    'analysis': analysis, 
    'sentiment_analysis_result': sentiment_analysis_result
})